{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports X_train, y_train (160 K rows), X_test and y_test (40 K), which already have been pre-processed.\n",
    "\n",
    "Also imports train, which is the full 200 K rows, not pre-processed yet - other than being a join of transaction- and identity data.  \n",
    "\n",
    "The focus is on train, as pre-processing steps are compared here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_from_csv = True # Set to true to load .csv data and do some basic pre-processing (joining)\n",
    "n_rows = 200000 # Set to None to load all data (recommended: 200'000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale as preproc_scale\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the packages and versions to requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seaborn==0.9.0\n",
      "scikit-learn==0.20.2\n",
      "plotly==4.3.0\n",
      "pandas==0.23.4\n",
      "numpy==1.17.2\n",
      "matplotlib==3.0.2\n"
     ]
    }
   ],
   "source": [
    "# Code from Stackoverflow\n",
    "# https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook\n",
    "\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to had\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))\n",
    "with open('requirements.txt', 'wt') as f:\n",
    "    [f.write(\"{}=={}\\n\".format(*r)) for r in requirements]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_isoF(X_train, y_train, X_test=None, y_test=None, max_samples=1024, feature_list=None):\n",
    "    if not feature_list is None:\n",
    "        X_train, X_test = X_train[feature_list], X_test[feature_list]\n",
    "    ifo = IsolationForest(n_estimators=50, max_samples=max_samples)\n",
    "    ifo.fit(X_train)\n",
    "    y_pred_ifo = ifo.decision_function(X_train)\n",
    "    print('AUC Score on Train: {:.3f}'.format(roc_auc_score(y_train, -y_pred_ifo)))\n",
    "    if X_test is None:\n",
    "        return ifo\n",
    "    y_pred_ifo_test = ifo.decision_function(X_test)    \n",
    "    print('AUC Score on Test: {:.3f}'.format(roc_auc_score(y_test, -y_pred_ifo_test)))\n",
    "    return ifo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median_imputation(df, median_impute_limit=0.95, impute_val=-999):\n",
    "    \"\"\" inf/nan Values that occur more often than median_impute_limit are imputed with the median\n",
    "    when less often, they are imputed by impute_val. \n",
    "    Set median_impute_limit to 0 to always do median imputation\n",
    "    \"\"\"\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    for col in df.columns:\n",
    "        if not df[col].dtype == 'object':\n",
    "            mean_nan = df[col].isna().mean()\n",
    "            if mean_nan > median_impute_limit: # then, impute by median\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            elif mean_nan > 0 and mean_nan <= median_impute_limit:\n",
    "                df[col] = df[col].fillna(impute_val)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_encoding(df, columns, test_df=None):\n",
    "    # %%time\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        if not test_df is None:\n",
    "            le.fit(list(df[col].astype(str).values) + list(test_df[col].astype(str).values))          \n",
    "            df[col] = le.transform(list(df[col].astype(str).values))\n",
    "            test_df[col] = le.transform(list(test_df[col].astype(str).values)) \n",
    "        else:\n",
    "            le.fit(list(df[col].astype(str).values))\n",
    "            df[col] = le.transform(list(df[col].astype(str).values))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\" function from Kaggle. Transforms the column data types to the smallest possible representation\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                #if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                #    df[col] = df[col].astype(np.int8)\n",
    "                #elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                #    df[col] = df[col].astype(np.int16)\n",
    "                if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                #    df[col] = df[col].astype(np.float16)\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased from {:5.2f} to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and basic pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded here: \n",
    "\n",
    "https://www.kaggle.com/c/ieee-fraud-detection\n",
    "\n",
    "Note: there are two data sets that need to be joined: Transaction data, on which Identity data (which is oftentimes missing) is left-joined. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (200000, 435)\n",
      "Mem. usage decreased from 665.28 to 357.06 Mb (46.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = 'data/train.pkl'\n",
    "TRAXCOLUMNS_PATH = 'data/trax_columns.pkl\n",
    "if load_from_csv:\n",
    "    data_transaction = pd.read_csv('data/ieee-fraud-detection/train_transaction.csv', nrows=n_rows)\n",
    "\n",
    "    data_identity = pd.read_csv('data/ieee-fraud-detection/train_identity.csv')\n",
    "    data_identity['has_id'] = 1 # to identify those that had identity info \n",
    "    \n",
    "    train = data_transaction.merge(data_identity, on='TransactionID', how='left')\n",
    "    train['has_id'] = train['has_id'].fillna(0)\n",
    "\n",
    "    identity_cols = data_identity.columns\n",
    "    transaction_cols = data_transaction.columns\n",
    "    transaction_cols.to_pickle(TRAXCOLUMNS_PATH)\n",
    "    del data_transaction, data_identity\n",
    "    print('train shape: {}'.format(train.shape))\n",
    "    train = reduce_mem_usage(train)\n",
    "    train.to_pickle(TRAIN_DATA_PATH)\n",
    "else:\n",
    "    base_path='basic_data'\n",
    "    train = pd.read_pickle(TRAIN_DATA_PATH)\n",
    "    # trax_features: a Series containing the name of features from the transactions data\n",
    "    transaction_cols = pd.read_pickle(TRAXCOLUMNS_PATH) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isFraud.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.iinfo(np.int16).max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.isFraud.to_pickle('y_train_full.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_groups = {'card': [c for c in train.columns if c.startswith('card')],\n",
    "                   'addr': [c for c in train.columns if c.startswith('addr')],\n",
    "                   'dist1': ['dist1', 'dist2'],\n",
    "                   'C' : [c for c in train.columns if c.startswith('C')],\n",
    "                   'D' : [c for c in train.columns if c.lstrip('D').isnumeric()],\n",
    "                   'M' : [c for c in train.columns if c.startswith('M')],\n",
    "                   'id' : [c for c in train.columns if c.startswith('id')],   \n",
    "                   'V': [c for c in train.columns if c.startswith('V')],\n",
    "                   'trans': [c for c in train.columns if c.startswith('Trans')]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcols_in_groups = []\n",
    "for col_group in column_groups.values():\n",
    "    allcols_in_groups += col_group\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train.columns) - set(allcols_in_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Kaggle forum:\n",
    "\n",
    "card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "\n",
    "All categorical! (according to the competition host)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(5).loc[:, column_groups['card']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, column_groups['card']].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, column_groups['card']].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "card_encoded = label_encoding(train[column_groups['card']], column_groups['card'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dist1  Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist: distance\n",
    "\"distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['dist1']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['dist1']].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['dist1']].sample(5, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[column_groups['dist1']].isna().mean()\n",
    "dist_cols = pd.concat((train[['dist1']].fillna(train[['dist1']].median()), \n",
    "                        train[['dist1']].isna().astype(int).rename(columns={'dist1': 'dist1_nan'}),\n",
    "                      train[['dist2']].fillna(train[['dist2']].median()), \n",
    "                        train[['dist2']].isna().astype(int).rename(columns={'dist2': 'dist2_nan'})), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: these fields are mostly empty. Indicator column was added for both\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address Columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(10, random_state=1)[column_groups['addr']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['addr']].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[column_groups['addr']].isna().sum()\n",
    "addr2 = train['addr2'].fillna(train['addr2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['addr2'].mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Indicator for missing values may be useful\n",
    "- According to the description, these columns are categorical. Do not seem very useful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C- columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['C']].dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train[column_groups['C']].corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do PCA on the C columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_original = preproc_scale(train[column_groups['C']])\n",
    "pca = PCA(n_components=3)\n",
    "C_transformed = pca.fit_transform(C_original)\n",
    "\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1) # NB: 3 variables is plenty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Observation: C columns can be effectively be represented by 3 PCA coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D- columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D1-D15: timedelta, such as days between previous transaction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ORIGINAL D\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(train.TransactionDT,train.D15, s=2)\n",
    "plt.title('Original D15')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('D15')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['D']].dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train[column_groups['D']].corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_groups['D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['D']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_imputed = median_imputation(train[column_groups['D']].copy(), median_impute_limit=0)\n",
    "D_original = preproc_scale(D_imputed)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "D_transformed = pca.fit_transform(D_original)\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1) # NB: 3 variables is plenty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Observation: D columns can be effectively be represented by 10 PCA coefficients (91%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V- columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['V']].dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['V']].max().value_counts().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train[column_groups['V'][:20]].corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_original = median_imputation(train[column_groups['V']], median_impute_limit=0)\n",
    "V_original = preproc_scale(V_original)\n",
    "pca = PCA(n_components=25, whiten=True)\n",
    "V_transformed = pca.fit_transform(V_original)\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['V']].isna().sum().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do PCA on the \"NaN-ness\" of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_nans = train[column_groups['V']].isna().astype(int)\n",
    "pca = PCA(n_components=3)\n",
    "V_nans_transformed = pca.fit_transform(V_nans)\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With merely 3 components, more than 90% of variance is explained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "- There are about 330 V columns, with a lot of NaN's (~40% is missing)\n",
    "- These can be compressed to about 25 PCA coefficients (75% explained variance) if they are median-imputed\n",
    "- The rows with NaN's are equal for many V-columns\n",
    "- The NaN-ness can be efficiently PCA-compressed to 3 columns (93%) or perfectly reconstructed with 10 components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M- columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M1-M9: match, such as names on card and address, etc.\n",
    "Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['M']].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['M']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['M']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_encoded = label_encoding(train[column_groups['M']], column_groups['M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(M_encoded.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_transformed = preproc_scale(M_encoded)\n",
    "pca = PCA(n_components=3, whiten=True)\n",
    "M_transformed = pca.fit_transform(M_transformed)\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[column_groups['V']].isna().sum().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do PCA on the \"NaN-ness\" of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_nans = train[column_groups['V']].isna().astype(int)\n",
    "pca = PCA(n_components=3)\n",
    "V_nans_transformed = pca.fit_transform(V_nans)\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransactionAMT: transaction payment amount in USD\n",
    "\n",
    "“Some of the transaction amounts have three decimal places to the right of the decimal point. There seems to be a link to three decimal places and a blank addr1 and addr2 field. Is it possible that these are foreign transactions and that, for example, the 75.887 in row 12 is the result of multiplying a foreign currency amount by an exchange rate?”\n",
    "\n",
    "TransactionDT --> get the time in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_groups['trans']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transaction_hour = (train.TransactionDT % (60*60*24))/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['addr2'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Outlier Detection, Numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outlier_scores(scores):\n",
    "    roc_score = roc_auc_score(train.isFraud, scores)\n",
    "    classify_results = pd.DataFrame(data=pd.concat((train.isFraud, pd.Series(scores)), axis=1))\n",
    "    classify_results.rename(columns={0:'score'}, inplace=True)\n",
    "    sns.kdeplot(classify_results.loc[classify_results.isFraud==0, 'score'], label='negatives', shade=True, bw=0.01)\n",
    "    sns.kdeplot(classify_results.loc[classify_results.isFraud==1, 'score'], label='positives', shade=True, bw=0.01)\n",
    "    plt.title('AUC: {:.3f}'.format(roc_score))\n",
    "    plt.xlabel('Score');\n",
    "    return classify_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_N(scores, N=100):\n",
    "    N = min(N, len(scores)) \n",
    "    classify_results = pd.DataFrame(data=pd.concat((train.isFraud, pd.Series(scores)), axis=1))\n",
    "    classify_results.rename(columns={0:'score'}, inplace=True)\n",
    "    classify_results = classify_results.sort_values(by='score', ascending=False)[:N]\n",
    "    Npos_in_N = classify_results['isFraud'].sum()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 2))\n",
    "    ims = ax.imshow(np.reshape(classify_results.isFraud.values, [1, -1]), extent=[-0.5, N, N/50, -0.5])\n",
    "    ax.yaxis.set_visible(False)\n",
    "    # ax.xaxis.set_ticklabels\n",
    "    plt.colorbar(ims)    \n",
    "    plt.xlabel('Outlier rank [-]')\n",
    "    plt.title(f'Number of positives found: {Npos_in_N} (P@Rank{N}: {Npos_in_N/N:.1%})')\n",
    "    #plt.show()\n",
    "    return classify_results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?sns.kdeplot(classify_results.loc[classify_results.isFraud==0, 'score'], label='negatives', shade=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Time and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hour and Region\n",
    "X_hour_region = pd.concat((transaction_hour, addr2), axis=1)\n",
    "isof = train_test_isoF(X_hour_region, train.isFraud)\n",
    "scores = - isof.decision_function(X_hour_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sc = plt.scatter(transaction_hour, addr2, c=scores, cmap=cmap)\n",
    "fig.colorbar(sc)\n",
    "plt.title('Isolation Forest outliers')\n",
    "ax.set_xlabel('Time [sec]')\n",
    "ax.set_ylabel('Region [-]')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_outlier_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_top_N(scores, N=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Hour and region \"mismatches\" are only a weak indicator for fraud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) V columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Untransformed ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isof = train_test_isoF(V_original, train.isFraud)\n",
    "scores = - isof.decision_function(V_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_results = plot_outlier_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_top_N(scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maha_score = np.mean(V_original ** 2, axis=1)\n",
    "maha_score = np.log(1 + maha_score)  / 10\n",
    "classify_results = plot_outlier_scores(maha_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_top_N(maha_score, N=100)\n",
    "res = plot_top_N(scores, N=1000)\n",
    "res = plot_top_N(scores, N=800000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. PCA Transformed ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(V_transformed, train.isFraud)\n",
    "scores = - isof.decision_function(V_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_results = plot_outlier_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_top_N(scores, N=100)\n",
    "res = plot_top_N(scores, N=500)\n",
    "res = plot_top_N(scores, N=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative: calculate the Mahalonobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maha_score = np.mean(V_transformed ** 2, axis=1)\n",
    "maha_score = np.log(1 + maha_score)  / 10\n",
    "classify_results = plot_outlier_scores(maha_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_top_N(maha_score, N=100)\n",
    "res = plot_top_N(maha_score, N=1000)\n",
    "res = plot_top_N(maha_score, N=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) V-nan columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(V_nans, train.isFraud)\n",
    "scores = - isof.decision_function(V_nans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(V_nans_transformed, train.isFraud)\n",
    "scores = - isof.decision_function(V_nans_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) C-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(train[column_groups['C']], train.isFraud)\n",
    "# scores = - isof.decision_function(C_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(C_transformed, train.isFraud)\n",
    "#scores = - isof.decision_function(C_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) M-columns (categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(M_encoded, train.isFraud)\n",
    "scores = - isof.decision_function(M_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(M_transformed, train.isFraud)\n",
    "scores = - isof.decision_function(M_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) card-columns (categorical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. All columns **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(card_encoded, train.isFraud)\n",
    "scores = - isof.decision_function(card_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_results = plot_outlier_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. All but first column **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(card_encoded.iloc[:, 1:], train.isFraud)\n",
    "scores = - isof.decision_function(card_encoded.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_results = plot_outlier_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Combining the best groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = np.concatenate((V_transformed, card_encoded.iloc[:, 1:].values, train[column_groups['C']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = train_test_isoF(data_combined, train.isFraud)\n",
    "scores = - isof.decision_function(data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_results = plot_outlier_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_top_N(scores, N=100)\n",
    "res = plot_top_N(scores, N=1000)\n",
    "res = plot_top_N(scores, N=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
