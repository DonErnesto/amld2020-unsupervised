{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/amld2021-unsupervised/blob/master/notebooks/exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ek6rUiPXRC-"
   },
   "source": [
    "## Exercise: Demonstration of several algorithms on the Pen Digits dataset\n",
    "\n",
    "Exercises using the Pen Digits Dataset: https://www.dbs.ifi.lmu.de/research/outlier-evaluation/DAMI/literature/PenDigits/PenDigits_v01.html\n",
    "\n",
    "The data is already present in .csv format (load the data below).\n",
    "This data set was created by recording the writing pattern of digits on a digital writing pad. The digit \"4\" is downsampled to only 20 instances (instead of ~1000 for the other points), making it an outlier. \n",
    "\n",
    "Note that (unlike MNIST) the features are not pixel values, but are x,y subsampled coordinate pairs, for a total of 8 pairs. \n",
    "\n",
    "This dataset is small and simple: it has only numeric features and no NaN's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsyJ1d24Z9Je"
   },
   "source": [
    "# Package installing and data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab, need to get data and install libraries..')\n",
    "    data_path = './'\n",
    "    # Now only load the required files...\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/amld2021-unsupervised/master/notebooks/outlierutils.py\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/amld2021-unsupervised/master/data/x_pendigits.csv\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/amld2021-unsupervised/master/data/y_pendigits.csv\n",
    "    !pip install --upgrade pyod\n",
    "else:\n",
    "    print('Not running on CoLab, data and libraries are already present')\n",
    "    data_path = '../data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpkjzX_2XRDD"
   },
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_MZmQrSTXRDF",
    "outputId": "ef404de3-cb2b-4f31-88cb-2b88935efb6e"
   },
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDdOJyUvZaeZ"
   },
   "outputs": [],
   "source": [
    "from outlierutils import plot_top_N, plot_outlier_scores # For easy plotting and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9DYhZQRXRDQ"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x_pen_raw = pd.read_csv(os.path.join(data_path, 'x_pendigits.csv'))\n",
    "y_pen = pd.read_csv(os.path.join(data_path, 'y_pendigits.csv'))['outlier']\n",
    "\n",
    "# Scale and put again into a DataFrame\n",
    "sc = StandardScaler()\n",
    "x_pen = pd.DataFrame(data=sc.fit_transform(x_pen_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of points: {len(y_pen)}')\n",
    "print(f'Number of positives: {y_pen.sum()} ({y_pen.mean():.3%})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_outlier = y_pen[y_pen == 0]\n",
    "outlier = y_pen[y_pen == 1]  \n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "axs = axs.ravel()\n",
    "for ax, i in enumerate([15, 4, 1, 8, 1234]):\n",
    "    xcoor = x_pen_raw.iloc[non_outlier.index[i], :].values.reshape([8,2])\n",
    "    axs[ax].plot(xcoor[:,0], xcoor[:,1],'ro-')\n",
    "    axs[ax].axis('equal')\n",
    "    axs[ax].axis('off')\n",
    "fig.suptitle('Non outlier [any digit except 4s]')\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "axs = axs.ravel()\n",
    "for ax, i in enumerate(range(5)):\n",
    "    xcoor = x_pen_raw.iloc[outlier.index[i], :].values.reshape([8,2])\n",
    "    axs[ax].plot(xcoor[:,0], xcoor[:,1],'ro-')\n",
    "    axs[ax].axis('equal')\n",
    "    axs[ax].axis('off')\n",
    "fig.suptitle('Outlier [4s digits]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the x, y coordinates of the pen strokes when writing the digits. The non-outliers show numbers from 0-9 (except 4), and the outliers represent the digit 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Usage of plotting functions\n",
    "\n",
    "See examples how to plot the conditional scores and the top-N ranking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with random data\n",
    "N_top = 1000\n",
    "y_true_, scores_ = np.random.choice([0, 1], N_top), np.random.uniform(size=N_top)\n",
    "results = plot_outlier_scores(y_true=y_true_, \n",
    "                              scores=scores_, \n",
    "                              bw_adjust=0.5, \n",
    "                              title='Example.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next plot shows the true labels of the N points with the highest outlier scores.\n",
    "# More yellow is better!\n",
    "\n",
    "results = plot_top_N(y_true=y_true_, scores=scores_, N=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "t-SNE of large datasets may take a long time to compute. The next piece of code will downsample the negatives, while retaining all positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "N_downsample = 3000\n",
    "assert x_pen.index.equals(y_pen.index), 'Error, indexes differ. Reset them to continue'\n",
    "x_downsampled = pd.concat((x_pen[y_pen==0].sample(N_downsample - int(y_pen.sum()), random_state=1),\n",
    "                           x_pen[y_pen==1]), \n",
    "                          axis=0).sample(frac=1, random_state=1)\n",
    "y_downsampled = y_pen[x_downsampled.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 0. \n",
    "Reduce the dimensionality with T-SNE, and visualize the positive and negative class in a scatter plot. \n",
    "What do you observe?\n",
    "\n",
    "**Hint**: To get help for a function or class, run `?<object>`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_N_TSNE = 4000 #Avoid overly long computation times with TSNE. Values < 4000 recommended \n",
    "neg = y_downsampled == 0\n",
    "pos = y_downsampled == 1\n",
    "\n",
    "assert len(x_downsampled) <= MAX_N_TSNE, f'Using a dataset with more than {MAX_N_TSNE} points is not recommended'\n",
    "X_2D = TSNE().fit_transform(x_downsampled) # transform to 2-D space for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.scatter(X_2D[pos, 0], X_2D[pos, 1], c=[[0.8, 0.4, 0.4],], marker='x', s=120, label='Positive')\n",
    "ax.scatter(X_2D[neg, 0], X_2D[neg, 1], c=[[0.2, 0.3, 0.9],], marker='o', s=10, label='Negative')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del x_downsampled, y_downsampled # To avoid using the wrong data later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance\n",
    "\n",
    "Using `EmpiricalCovariance`, or `MinCovDet` (a robust estimator), do a `.fit()` to fit the covariance matrix. \n",
    "Determine the distance with `.mahalanobis()` and use this as outlier score. Get the Area Under the ROC-Curve, PR-curve and Precision@100 using `plot_outlier_scores` and `plot_top_N`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet, EmpiricalCovariance\n",
    "cov_ = EmpiricalCovariance().fit(x_pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = cov_.mahalanobis(x_pen)\n",
    "#plot_outlier_scores(y_pen, dist)\n",
    "plot_top_N(y_pen, dist, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM\n",
    "\n",
    "Using `GaussianMixture`, with a reasonable value for n_components and `covariance_type=full`, do a `.fit()` and `.score_samples()` to get the log-probability of each sample. The negative log-probability will be the outlier score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# gmm = GaussianMixture(n_components=xxxx, covariance_type='full', random_state=1) # try also spherical\n",
    "# gmm.fit(x_pen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1. \n",
    "Which algorithm performed better, GMM or Mahalonobis? Why do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 2.1\n",
    "With the scikit-learn NearestNeighbors class, determine the probability of the nearest neighbour of a point being an outlier, conditional on the class membership of that point. If the nearest neighbour of the outliers is often an outlier, we can conclude that the outliers form one (or more than one) cluster. Compare this result with the observations on the t-SNE visualization\n",
    " \n",
    "The code provided in the next cell calculates the indices of the nearest neighbour of all points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# clf_nn = NearestNeighbors(n_neighbors=10)\n",
    "# clf_nn.fit(x_pen)\n",
    "# # the function .kneighbors() returns one array with distances and one array with indexes of neighbours:\n",
    "# distances, indices = clf_nn.kneighbors(x_pen)\n",
    "# # for each point, look at the index of the closest point (the 0-th is the point itself, so we look for the point at index 1):\n",
    "# indices_nearest_neighbor = indices[:, 1]\n",
    "\n",
    "# # find out how many of the nearest neighbors of each outlier are outliers themselves.\n",
    "# # In other words, find out the conditional label of the nearest neighbors of each point, conditioned on the point being an outlier.\n",
    "# # use the numpy mask y_pen==1 as needed to consider just the outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 2.2\n",
    "Use pyod's KNN class to detect the outliers. Use `method=largest` (the default) and guess a reasonable value for `n_neighbors` based on the insights from t-SNE and the previous question. For the scores, you can use the `.decision_scores_` attributes of the fitted KNN model. This is simply a vector of distances.\n",
    "\n",
    "Plot the conditional score curves and the top-100 results, using `plot_outlier_scores` and `plot_top_N`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "# clf_knn = KNN(method='median', n_neighbors=xxx)\n",
    "# clf_knn.fit(x_pen)\n",
    "## get the prediction label and outlier scores of the training data\n",
    "# scores_knn = clf_knn.decision_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 3.\n",
    "\n",
    "Vary `n_neighbors`, how does it affect:\n",
    "- AUC-ROC (area under the curve (AUC) in the Receiver Operating Characteristic (ROC) (the higher the better, 0.5 for a random classifier)\n",
    "- precision@100, which is the fraction of outliers among the highest 100 scoring points\n",
    "\n",
    "**HINT**: \n",
    "1. Use clf.decision_scores_ as in the previous step\n",
    "2. Run `roc_auc_score?` to see the docs for calculating AUC-ROC\n",
    "3. Use the provided function `calc_precision_at_100` to calculate the precision@100 for a scores vector `scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calc_precision_at_100(scores):\n",
    "    y_pen[np.argsort(scores)][::-1][:100].mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOF \n",
    "\n",
    "Note that the LOF algorithm compares the \"reachability density\" of a point to its k nearest neighbours, compared to that same density of its nearest neighbours.\n",
    "\n",
    "Also here, we will use a pyod class. Therefore, we can use the same method- and attribute names. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 4.\n",
    "Considering the concept that underlies LOF, and the results from the t-SNE/Nearest neighbour analysis, do you expect LOF to do better than KNN with n_neighbours=10? Why?\n",
    "\n",
    "#### Q 5.\n",
    "Plot the scoring curves for a few values of n_neighbours. What is a good value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.lof import LOF\n",
    "# lof = LOF(n_neighbors=n_neighbours, contamination=0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 6. \n",
    "What disadvantage of Isolation Forest do you see compared to the previous algorithms?\n",
    "\n",
    "Run an `IsolationForest`analysis with a reasonable set of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "#ifo = IForest(behaviour='new',n_estimators=10, max_samples=512)\n",
    "#ifo.fit(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an alternative you can use sklearn\n",
    "# from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional question)\n",
    "Compare the scatter in AUC scores by running ten times the Isolation Forest, with different `random_state`s, for `n_estimators`=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 7.\n",
    "What number of components would you estimate to be suffificient? How may it be determined?\n",
    "\n",
    "#### Q 8.\n",
    "Determine the Euclidean reconstruction error, by first transforming the data, and then applying the inverse transform. What scores do you get?\n",
    "\n",
    "**Use the sci-kit learn implementation, rather than pyod's PCA class. This one seems not to be implemented well**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pyod.models.pca import PCA as pyod_PCA\n",
    "# pca = PCA(n_components=...)\n",
    "# pca_tf = ... \n",
    "# x_pen_recon = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_recon = ((x_pen - x_pen_recon)**2).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the autoencoder with a bottleneck size that worked well for PCA. Run for ~10-15 epochs and look at AUC score. \n",
    "Many different configurations (number of hidden layers, number of neurons) may be used, but pick one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 9. \n",
    "What output activation do you think will work best? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "\n",
    "clf = AutoEncoder(\n",
    "    hidden_neurons=[xx, xx, xx], # Choose size here!\n",
    "    hidden_activation='elu',\n",
    "    output_activation='xx', # Choose an activation ('linear', 'sigmoid', 'relu', 'elu' are some possibilities)\n",
    "    optimizer='adam',\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    dropout_rate=0.0, #may not be needed here\n",
    "    l2_regularizer=0.0,\n",
    "    validation_size=0.1,\n",
    "    preprocessing=False, #NB: this uses sklearn's StandardScaler\n",
    "    verbose=1,\n",
    "    random_state=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(x_pen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 10.\n",
    "\n",
    "- Which algorithm performed best?\n",
    "- Can it be reasonably \"tuned\" without having the labels available?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "exercises_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
